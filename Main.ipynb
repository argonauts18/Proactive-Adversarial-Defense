{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YGlCnh1elXfI"
   },
   "outputs": [],
   "source": [
    "# Install CLIP package\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import torch\n",
    "import clip\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from tqdm import tqdm\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K0fhw2y_lECr",
    "outputId": "243af841-5367-4500-a6a5-c58d2edd5012"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Files extracted to: /content/triggers/triggers/\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Path to ZIP file and extract location\n",
    "zip_path = \"/content/drive/My Drive/Adversarial Signs/Triggers_v2/triggers.zip\"\n",
    "extract_path = \"/content/triggers/triggers/\"\n",
    "\n",
    "# Extract the ZIP file\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "print(f\"Files extracted to: {extract_path}\")\n",
    "\n",
    "# Paths to output_images\n",
    "base_path = os.path.join(extract_path, \"output_images_cifar10\")\n",
    "clean_path = os.path.join(base_path, \"clean\")\n",
    "attack_folders = [\"badnets_pixels\", \"badnets_square\", \"l0_inv\", \"l2_inv\", \"trojan_sq\", \"trojan_wm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7akyGuc8lZhn",
    "outputId": "146011d1-4177-41f4-904e-6475bc15688f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training attack folders: ['badnets_square', 'l0_inv', 'l2_inv', 'trojan_sq', 'trojan_wm']\n",
      "Unseen attack folder: badnets_pixels\n"
     ]
    }
   ],
   "source": [
    "# Specify the unseen attack\n",
    "unseen_attack = \"trojan_wm\"\n",
    "train_attack_folders = [folder for folder in attack_folders if folder != unseen_attack]\n",
    "\n",
    "print(f\"Training attack folders: {train_attack_folders}\")\n",
    "print(f\"Unseen attack folder: {unseen_attack}\")\n",
    "\n",
    "# Prepare the dataset\n",
    "def prepare_clip_data():\n",
    "    print(\"Preparing data...\")\n",
    "\n",
    "    # Define paths to train and test folders\n",
    "    train_clean_path = os.path.join(base_path, \"train\", \"clean\")\n",
    "    test_clean_path = os.path.join(base_path, \"test\", \"clean\")\n",
    "\n",
    "    # Collect all train and test clean images\n",
    "    train_clean = [os.path.join(train_clean_path, img) for img in os.listdir(train_clean_path) if img.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    test_clean = [os.path.join(test_clean_path, img) for img in os.listdir(test_clean_path) if img.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    print(f\"Training clean images: {len(train_clean)}, Testing clean images: {len(test_clean)}\")\n",
    "\n",
    "    # Collect attack images for training\n",
    "    train_attacks = []\n",
    "    for folder in train_attack_folders:\n",
    "        attack_path = os.path.join(base_path, \"train\", folder)\n",
    "        attack_images = [os.path.join(attack_path, img) for img in os.listdir(attack_path) if img.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        sampled_attacks = random.sample(attack_images, len(train_clean) // len(train_attack_folders))  # Balance with clean images\n",
    "        train_attacks += sampled_attacks\n",
    "        print(f\"Sampled {len(sampled_attacks)} images from {folder} for training.\")\n",
    "\n",
    "    # Collect unseen attack images for testing\n",
    "    unseen_attack_path = os.path.join(base_path, \"test\", unseen_attack)\n",
    "    unseen_attack_images = [os.path.join(unseen_attack_path, img) for img in os.listdir(unseen_attack_path) if img.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    test_unseen = random.sample(unseen_attack_images, len(test_clean))  # Match clean test size\n",
    "    print(f\"Number of unseen attack images for testing: {len(test_unseen)}\")\n",
    "\n",
    "    # Combine data and labels\n",
    "    train_data = [(img, 0) for img in train_clean] + [(img, 1) for img in train_attacks]\n",
    "    test_data = [(img, 0) for img in test_clean] + [(img, 1) for img in test_unseen]\n",
    "\n",
    "    # Shuffle both datasets\n",
    "    random.shuffle(train_data)\n",
    "    random.shuffle(test_data)\n",
    "\n",
    "    print(f\"Training dataset size: {len(train_data)}, Testing dataset size: {len(test_data)}\")\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n8fN5dfBmrWr"
   },
   "outputs": [],
   "source": [
    "# Custom PyTorch dataset\n",
    "class CLIPDataset(Dataset):\n",
    "    def __init__(self, data, transform):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# Preprocessing for CLIP\n",
    "preprocess = Compose([\n",
    "    Resize((224, 224)),\n",
    "    CenterCrop(224),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "# Prepare datasets\n",
    "train_data, test_data = prepare_clip_data()\n",
    "\n",
    "train_dataset = CLIPDataset(train_data, transform=preprocess)\n",
    "test_dataset = CLIPDataset(test_data, transform=preprocess)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f\"Training data: {len(train_dataset)}, Testing data: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wsvF0MG1mIh3"
   },
   "outputs": [],
   "source": [
    "# Load the CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "model = model.float()\n",
    "\n",
    "# Define label mapping and prompts\n",
    "label_map = {0: \"clean\", 1: \"adversarial\"}\n",
    "full_prompts = [f\"a photo of {label_map[label]}\" for label in label_map.keys()]\n",
    "\n",
    "# Tokenize prompts\n",
    "full_token_ids = clip.tokenize(full_prompts).to(device)\n",
    "print(f\"Tokenized prompts: {full_prompts}\")\n",
    "\n",
    "# Get the token embeddings for the full prompts\n",
    "with torch.no_grad():\n",
    "    full_token_embeddings = model.token_embedding(full_token_ids).type(model.dtype)\n",
    "\n",
    "# Identify the prefix length\n",
    "prefix = \"a photo of\"\n",
    "prefix_token_ids = clip.tokenize(prefix).to(device)\n",
    "prefix_length = prefix_token_ids.ne(0).sum().item()\n",
    "print(f\"Prefix length: {prefix_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubCvB4NNrlQ9"
   },
   "outputs": [],
   "source": [
    "# Tokenize \"a photo of\" to extract embeddings\n",
    "prefix_tokens = clip.tokenize(\"a photo of\").to(device)\n",
    "\n",
    "# Extract the token embeddings for \"a photo of\"\n",
    "with torch.no_grad():\n",
    "    prefix_embeddings = model.token_embedding(prefix_tokens).squeeze(0)\n",
    "\n",
    "# Set the prefix embeddings as learnable parameters\n",
    "prefix_embeddings = nn.Parameter(prefix_embeddings[:prefix_length])  # Ensure correct prefix length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cT5ltwrtll0W"
   },
   "outputs": [],
   "source": [
    "# Freeze other parts of the text encoder\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.visual.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zuoFHaU6lz4O"
   },
   "outputs": [],
   "source": [
    "# Define the custom text encoder\n",
    "class CustomTextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model, prefix_embeddings, full_token_ids, prefix_length):\n",
    "        super(CustomTextEncoder, self).__init__()\n",
    "        self.clip_model = clip_model\n",
    "        self.prefix_embeddings = prefix_embeddings\n",
    "        self.full_token_ids = full_token_ids\n",
    "        self.prefix_length = prefix_length\n",
    "        self.token_embedding = clip_model.token_embedding\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "    def forward(self):\n",
    "        num_classes = self.full_token_ids.size(0)\n",
    "        token_embeddings = self.token_embedding(self.full_token_ids).type(self.dtype)\n",
    "        token_embeddings[:, :self.prefix_length, :] = self.prefix_embeddings.unsqueeze(0).expand(num_classes, -1, -1).type(self.dtype)\n",
    "        positional_embeddings = self.positional_embedding[:token_embeddings.size(1), :].type(self.dtype)\n",
    "        x = token_embeddings + positional_embeddings\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "        eos_indices = self.full_token_ids.argmax(dim=-1)\n",
    "        x = x[torch.arange(x.shape[0]), eos_indices] @ self.text_projection\n",
    "        return x\n",
    "\n",
    "# Initialize the custom text encoder\n",
    "text_encoder = CustomTextEncoder(model, prefix_embeddings, full_token_ids, prefix_length).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2kv-1EMll3EU"
   },
   "outputs": [],
   "source": [
    "# Define the classifier model\n",
    "class CLIPClassifier(nn.Module):\n",
    "    def __init__(self, clip_model, text_encoder):\n",
    "        super(CLIPClassifier, self).__init__()\n",
    "        self.clip_model = clip_model\n",
    "        self.text_encoder = text_encoder\n",
    "\n",
    "    def forward(self, images):\n",
    "        image_embeddings = self.clip_model.encode_image(images)\n",
    "        image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)\n",
    "        text_embeddings = self.text_encoder()\n",
    "        text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)\n",
    "        logits = 100.0 * image_embeddings @ text_embeddings.T\n",
    "        return logits\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "classifier_model = CLIPClassifier(model, text_encoder).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam([\n",
    "    {'params': classifier_model.text_encoder.prefix_embeddings, 'lr': 1e-5}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7iuvglbnQiWJ"
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    classifier_model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = classifier_model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Evaluation\n",
    "classifier_model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "correct, total = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        logits = classifier_model(images)\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "        # Collect predictions and labels for F1 score computation\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Compute accuracy\n",
    "        total += labels.size(0)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Calculate F1 score (macro or weighted based on your needs)\n",
    "f1 = f1_score(all_labels, all_predictions, average='macro')  # Use 'weighted' for weighted F1\n",
    "print(f\"F1 Score (Macro): {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xk7hBUw5ou6E"
   },
   "outputs": [],
   "source": [
    "# Evaluation loop (modified to extract embeddings)\n",
    "image_embeddings_list = []\n",
    "text_embeddings_list = []\n",
    "labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        logits = classifier_model(images)\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "        # Collect predictions and labels for F1 score computation\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Extract and normalize embeddings\n",
    "        image_embeddings = classifier_model.clip_model.encode_image(images)\n",
    "        image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)\n",
    "        text_embeddings = classifier_model.text_encoder()\n",
    "        text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Append embeddings and labels\n",
    "        image_embeddings_list.append(image_embeddings.cpu().numpy())\n",
    "        text_embeddings_list.append(text_embeddings.cpu().numpy())\n",
    "        labels_list.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Compute accuracy\n",
    "        total += labels.size(0)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"F1 Score (Macro): {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
